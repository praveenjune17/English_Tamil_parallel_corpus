{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Abstractive_summarization.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praveenjune17/English_Tamil_parallel_corpus/blob/master/Text_summarization/Abstractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN4KXd2GWvlz",
        "colab_type": "code",
        "outputId": "ec3b57fd-9435-4604-a848-dd2d27078335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUPGZQu9ONfu",
        "colab_type": "code",
        "outputId": "364765d3-e5f8-4c87-b059-1e95ab4c2a9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "!pip install rouge==0.3.2\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(100)\n",
        "import time\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.6)\n",
            "Requirement already satisfied: rouge==0.3.2 in /usr/local/lib/python3.6/dist-packages (0.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFsxrEiOFUoB",
        "colab_type": "code",
        "outputId": "e926d770-6917-4063-e56a-04dcccd9a744",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "!tf_upgrade_v2 \\\n",
        "  --infile /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/beam_search.py \\\n",
        "  --outfile /usr/local/lib/python3.6/dist-packages/tensor2tensor/utils/beam_search.py\n",
        "from tensor2tensor.utils.beam_search import beam_search\n",
        "from rouge import Rouge"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.0 Upgrade Script\n",
            "-----------------------------\n",
            "Converted 1 files\n",
            "Detected 0 issues that require attention\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Make sure to read the detailed log 'report.txt'\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I15hvcwHOTnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "examples, metadata = tfds.load('cnn_dailymail', with_info=True, as_supervised=True)\n",
        "train, val, test = examples['train'], examples['validation'], examples['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h313I2NrOYb-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for doc, summ in examples['train']:\n",
        "#   print('document summary',doc.numpy())\n",
        "#   print('summary', summ.numpy())\n",
        "#   break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkFCSj4L8DEF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# count=0\n",
        "# for doc, summ in examples['train']:\n",
        "#   count+=1\n",
        "#print(count)   # Number of examples available 287113"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE0mp3pMyLNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_examples = examples['train']\n",
        "val_examples = examples['validation']\n",
        "test_examples = examples['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-rDIIjHwg0k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
        "#     (doc.numpy() for doc, sum_r in train_examples), target_vocab_size=2**13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stgAlkxMBpTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer_en = tfds.features.text.SubwordTextEncoder.load_from_file('/content/vocab_file_summarization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5QYvzeRydXX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#tokenizer_en.save_to_file('/content/vocab_file_summarization')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9lPvguyxha3",
        "colab_type": "code",
        "outputId": "df83783c-601f-4f44-ddb2-3dd53a46bd9a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sample_string = 'Transformer is awesome.'\n",
        "\n",
        "tokenized_string = tokenizer_en.encode(sample_string)\n",
        "print ('Tokenized string is {}'.format(tokenized_string))\n",
        "\n",
        "original_string = tokenizer_en.decode(tokenized_string)\n",
        "print ('The original string: {}'.format(original_string))\n",
        "\n",
        "assert original_string == sample_string"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenized string is [8089, 1498, 8120, 201, 15, 2371, 3746, 277, 8051]\n",
            "The original string: Transformer is awesome.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpeoc_Bzxkp9",
        "colab_type": "code",
        "outputId": "459d188a-99a8-4daa-b949-148273e66262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "for ts in tokenized_string:\n",
        "  print ('{} ----> {}'.format(ts, tokenizer_en.decode([ts])))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8089 ----> T\n",
            "1498 ----> ran\n",
            "8120 ----> s\n",
            "201 ----> former \n",
            "15 ----> is \n",
            "2371 ----> aw\n",
            "3746 ----> eso\n",
            "277 ----> me\n",
            "8051 ----> .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFmW7_hAxoD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 287113\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIPOJx-kxr1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode(lang1, lang2):\n",
        "  lang1 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang1.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "\n",
        "  lang2 = [tokenizer_en.vocab_size] + tokenizer_en.encode(\n",
        "      lang2.numpy()) + [tokenizer_en.vocab_size+1]\n",
        "  \n",
        "  return lang1, lang2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P17pPT7nxywK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc_length = 512   #(200,75) worked,  , crashed at 230, 75 (275-305)\n",
        "summ_length = 70\n",
        "MAX_LENGTH = summ_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isr3_ymgx2dZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set threshold for document and  summary length\n",
        "def filter_max_length(x, y):\n",
        "  return tf.logical_and(tf.size(x) <= doc_length,\n",
        "                        tf.size(y) <= summ_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NXYJuerx6N8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tf_encode(doc, summary):\n",
        "  return tf.py_function(encode, [doc, summary], [tf.int64, tf.int64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJikNGzayFQ_",
        "colab_type": "code",
        "outputId": "6e69cd66-c35a-4263-f9d9-01d7128a8727",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_dataset = train_examples.map(tf_encode)\n",
        "train_dataset = train_dataset.filter(filter_max_length)\n",
        "# cache the dataset to memory to get a speedup while reading from it.\n",
        "#print('Number of records in train set after filter', sum(1 for _, _ in train_dataset)) # \n",
        "# 835 recs with doc_length = 180 & summ_length = 75\n",
        "# 3225 recs with doc_length = 250 & summ_length = 75\n",
        "# 2250 recs with doc_length = 225 & summ_length = 75\n",
        "# 1350 recs with doc_length = 240 & summ_length = 50\n",
        "# 1488 recs with doc_length = 220 & summ_length = 60\n",
        "# 29176 recs with doc_length = 512 and summ_length = 70 (validation set = 1409)\n",
        "train_dataset = train_dataset.cache()\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE, seed = 100).padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "val_dataset = val_examples.map(tf_encode)\n",
        "val_dataset = val_dataset.filter(filter_max_length)\n",
        "#print('Number of records in train set after filter', sum(1 for _, _ in val_dataset))\n",
        "# .padded_batch(\n",
        "#     BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "val_dataset = val_dataset.padded_batch(\n",
        "    BATCH_SIZE, padded_shapes=([-1], [-1]))\n",
        "val_dataset = val_dataset.prefetch(tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/data/util/random_seed.py:58: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jsJDjSdywPz",
        "colab_type": "code",
        "outputId": "812ce86f-b585-418c-9380-6df99b5950cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "doc_batch, summar_batch = next(iter(val_dataset))\n",
        "doc_batch, summar_batch"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: id=161628, shape=(64, 509), dtype=int64, numpy=\n",
              " array([[8261,    1, 6082, ...,    0,    0,    0],\n",
              "        [8261,  435, 5578, ...,    0,    0,    0],\n",
              "        [8261,    5, 2838, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8261,   75, 1343, ...,    0,    0,    0],\n",
              "        [8261, 5182,  238, ...,    0,    0,    0],\n",
              "        [8261,    5, 1035, ...,    0,    0,    0]])>,\n",
              " <tf.Tensor: id=161629, shape=(64, 70), dtype=int64, numpy=\n",
              " array([[8261,  423,    5, ...,    0,    0,    0],\n",
              "        [8261,  435, 5578, ...,    0,    0,    0],\n",
              "        [8261,    5, 2838, ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [8261, 2475, 1064, ...,    0,    0,    0],\n",
              "        [8261, 5182, 2038, ...,  228,  344, 8262],\n",
              "        [8261,  463, 8114, ...,    0,    0,    0]])>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5E_HHOPyG11",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates\n",
        "def positional_encoding(position, d_model):\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "  \n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "  \n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    \n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    \n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "  \n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "  \n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "    \n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "  \n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "    \n",
        "    assert d_model % self.num_heads == 0\n",
        "    \n",
        "    self.depth = d_model // self.num_heads\n",
        "    \n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "    \n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "        \n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "    \n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "    \n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "        \n",
        "    return output, attention_weights\n",
        "  \n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n",
        "\n",
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "    attn_output = self.dropout1(attn_output, training=training)\n",
        "    out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.dropout2(ffn_output, training=training)\n",
        "    out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "    \n",
        "    return out2\n",
        "  \n",
        "  \n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        " \n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "    # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "    attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn1 = self.dropout1(attn1, training=training)\n",
        "    out1 = self.layernorm1(attn1 + x)\n",
        "    \n",
        "    attn2, attn_weights_block2 = self.mha2(\n",
        "        enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "    attn2 = self.dropout2(attn2, training=training)\n",
        "    out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "    ffn_output = self.dropout3(ffn_output, training=training)\n",
        "    out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "    \n",
        "    return out3, attn_weights_block1, attn_weights_block2\n",
        "  \n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(input_vocab_size, self.d_model)\n",
        "    \n",
        "    \n",
        "    self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "  \n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "        \n",
        "  def call(self, x, training, mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    \n",
        "    # adding embedding and position encoding.\n",
        "    x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "    x = self.dropout(x, training=training)\n",
        "    \n",
        "    for i in range(self.num_layers):\n",
        "      x = self.enc_layers[i](x, training, mask)\n",
        "    \n",
        "    return x  # (batch_size, input_seq_len, d_model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NBp8qp4zTe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, \n",
        "               rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "    self.pos_encoding = positional_encoding(target_vocab_size, d_model)\n",
        "    \n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    seq_len = tf.shape(x)[1]\n",
        "    attention_weights = {}\n",
        "    \n",
        "    x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x += self.pos_encoding[:, :seq_len, :]\n",
        "    \n",
        "    x = self.dropout(x, training=training)\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "      \n",
        "      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsXeC45vzVGe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, \n",
        "               target_vocab_size, rate=0.1):\n",
        "    super(Transformer, self).__init__()\n",
        "\n",
        "    self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "                           input_vocab_size, rate)\n",
        "\n",
        "    self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "                           target_vocab_size, rate)\n",
        "\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "    \n",
        "    self.generator_vec = tf.keras.layers.Dense(1, activation='sigmoid' )\n",
        "    \n",
        "  def call(self, inp, tar, training, enc_padding_mask, \n",
        "           look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "    enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "    \n",
        "    # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "    dec_output, attention_weights = self.decoder(\n",
        "        tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "    \n",
        "    final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "    #print((attention_weights['decoder_layer{}_block2'.format(num_layers)]).shape)\n",
        "    return final_output, attention_weights, dec_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuGNu3GSBfI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HL11DAnp2HT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # add a dense layer (dff, 1) with sigmoid activation \n",
        "\n",
        "# # take mean of all the attention heads\n",
        "# updates = tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) #(batch_size, tar_seq_len, inp_seq_len)\n",
        "# updates = tf.transpose(updates, perm=[2 ,1, 0]) #(input_seq_len, tar_seq, batch)\n",
        "# tar_seq_len = updates.shape[1]\n",
        "# batch_size = updates.shape[-1]\n",
        "# shape = tf.constant([target_vocab_size, tar_seq_len, batch_size]) #rename batch_size when training \n",
        "# indices = tf.transpose(encoder_input)\n",
        "# copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "# copy_logits = tf.transpose(copy_logits_, perm=[2, 1, 0])\n",
        "# logits = gen_logits + copy_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRdjbOR6G9I3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "#                           input_vocab_size, target_vocab_size, dropout_rate)\n",
        "# # tar_inp = tar[:, :-1]\n",
        "# # tar_real = tar[:, 1:]\n",
        "\n",
        "# # enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "# # predictions, attention_weights, dec_output = transformer(inp, tar_inp, \n",
        "# #                                  True, \n",
        "# #                                  enc_padding_mask, \n",
        "# #                                  combined_mask, \n",
        "# #                                  dec_padding_mask,\n",
        "# #                                  enable_copy=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oNS3ttOjQAc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_layers = 4  #number of transformer blocks\n",
        "# d_model = 768   #the projected word vector dimension\n",
        "# dff = 512       #\n",
        "# num_heads = 12 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0UVxW5l2bVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # num_layers = 3  #number of transformer blocks\n",
        "# # d_model = 768   #the projected word vector dimension\n",
        "# # dff = 512       #\n",
        "# # num_heads = 12   #the number of heads in the multi-headed attention unit\n",
        "# Epoch 66 Batch 0 Loss 0.0015 Accuracy 0.9074\n",
        "# Epoch 66 Loss 0.0107 Accuracy 0.8711\n",
        "# Time taken for 1 epoch: 60.90722155570984 secs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idXEDepYzYC4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# num_layers = 8\n",
        "# d_model = 768\n",
        "# dff = 2048\n",
        "# num_heads = 8\n",
        "\n",
        "# input_vocab_size = tokenizer_en.vocab_size + 2\n",
        "# target_vocab_size = input_vocab_size\n",
        "# dropout_rate = 0.1\n",
        "num_layers = 3  #number of transformer blocks\n",
        "d_model = 512   #the projected word vector dimension\n",
        "dff = 512       #feed forward/filter size (in BERT its 4*d_model)\n",
        "num_heads = 8   #the number of heads in the multi-headed attention unit\n",
        "\n",
        "input_vocab_size = tokenizer_en.vocab_size + 2\n",
        "target_vocab_size = input_vocab_size\n",
        "dropout_rate = 0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9a_rVPmzgVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "    \n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "    \n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "    \n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyVPhfWWzjNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgPECkXSzjsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNhhmCFAzmRQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "  \n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUKVz4Yuzntc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n",
        "    name='train_accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYg-AA8LzoEs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformer = Transformer(num_layers, d_model, num_heads, dff,\n",
        "                          input_vocab_size, target_vocab_size, dropout_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNY5_LvPzr9U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "  # Encoder padding mask\n",
        "  enc_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 2nd attention block in the decoder.\n",
        "  # This padding mask is used to mask the encoder outputs.\n",
        "  dec_padding_mask = create_padding_mask(inp)\n",
        "  \n",
        "  # Used in the 1st attention block in the decoder.\n",
        "  # It is used to pad and mask future tokens in the input received by \n",
        "  # the decoder.\n",
        "  look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "  dec_target_padding_mask = create_padding_mask(tar)\n",
        "  combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "  \n",
        "  return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_wsWIUEztbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checkpoint_path = \"/content/drive/My Drive/Neural_machine_translation/Text_summarization_training\"\n",
        "checkpoint_path = \"/content/temp\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
        "                           optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print (ckpt_manager.latest_checkpoint, 'checkpoint restored!!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRQQ0jgyzvao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1bq9reoG2tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if enable_copy:\n",
        "#       p_gen = self.generator_vec(dec_output) # (batch_size, tar_seq_len, 1)\n",
        "#       vocab_dist_ = p_gen * final_output\n",
        "#       # mean of all the attention heads\n",
        "#       # updates shape (batch_size, tar_seq_len, inp_seq_len)\n",
        "#       #print(tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1).shape)\n",
        "#       updates = (1 - p_gen) * tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) \n",
        "#       #print(tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1))\n",
        "#       #print(updates.shape)\n",
        "#       updates = tf.transpose(updates, perm=[2, 1, 0])\n",
        "#       batch_size = updates.shape[-1]\n",
        "#       # since the target input is shifted right so reducing the summary length\n",
        "#       shape = tf.constant([target_vocab_size, summ_length-1, batch_size]) #rename batch_size when training \n",
        "#       #shape = tf.constant([BATCH_SIZE, summ_length-1, target_vocab_size]) #rename batch_size when training \n",
        "#       indices = tf.transpose(inp)#, perm=[1, 0])\n",
        "#       indices = tf.cast(indices, dtype=tf.int32)\n",
        "#       #indices = tf.expand_dims(indices, axis=1)\n",
        "#       print('updates {}'.format(updates.shape))\n",
        "#       print('indices {}'.format(indices.shape))\n",
        "#       print('shape {}'.format(shape))\n",
        "#       copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "#       #copy_logits = tf.transpose(copy_logits_, perm=[2, 1, 0])\n",
        "#       final_output = p_gen + copy_logits\n",
        "#       #final_output = tf.math.log(prob_output)\n",
        "#       # (batch_size, tar_seq, inp_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUsq6jT2BGNm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Generator(tf.keras.Model):\n",
        "#   def __init__(self):\n",
        "#     super(Generator, self).__init__()\n",
        "#     self.generator_vec = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#     self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "#   def call(self, dec_output, final_output, attention_weights, encoder_input, training):  \n",
        "#     p_gen = self.generator_vec(dec_output) # (batch_size, tar_seq_len, 1)\n",
        "#     vocab_dist_ = p_gen * final_output\n",
        "#     updates = (1 - p_gen) * tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) \n",
        "#     updates = tf.transpose(updates, perm=[2, 0, 1])\n",
        "#     #batch_size = updates.shape[-1]\n",
        "#     # since the target input is shifted right so reducing the summary length\n",
        "#     shape = tf.constant([target_vocab_size, BATCH_SIZE, summ_length-1]) #rename batch_size when training \n",
        "#     #shape = tf.constant([BATCH_SIZE, summ_length-1, target_vocab_size]) #rename batch_size when training \n",
        "#     #indices = tf.transpose(inp)#, perm=[1, 0])\n",
        "#     indices = tf.cast(encoder_input, dtype=tf.int32)\n",
        "#     #indices = tf.expand_dims(indices, axis=1)\n",
        "#     #updates = tf.expand_dims(updates, axis=3)\n",
        "#     #shape = tf.expand_dims(shape, axis=1)\n",
        "#     indices = tf.reshape(indices[0], [509, -1])\n",
        "#     print('updates {}'.format(updates.shape))\n",
        "#     print('indices {}'.format(indices.shape))\n",
        "#     print('shape {}'.format(shape))\n",
        "#     copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "#     copy_logits = tf.transpose(copy_logits_, perm=[1, 2, 0])\n",
        "#     combined_output = final_output + copy_logits\n",
        "#     final_output = self.final_layer(combined_output)  \n",
        "#     return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk0qwUopV9x3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8551a8a6-dd3c-4fa1-923b-48747a93c2c9"
      },
      "source": [
        "tf.constant([BATCH_SIZE, 70-1, target_vocab_size]) "
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=548181, shape=(3,), dtype=int32, numpy=array([  64,   69, 8263], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3zptADxzjeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(tf.keras.Model):\n",
        "  def __init__(self):\n",
        "    super(Generator, self).__init__()\n",
        "    self.generator_vec = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "  def call(self, dec_output, final_output, attention_weights, encoder_input, inp_shape, tar_shape, batch, training):  \n",
        "    #batch, summ_len = inp_shape\n",
        "    p_gen = self.generator_vec(dec_output) # (batch_size, tar_seq_len, 1)\n",
        "    vocab_dist_ = p_gen * final_output\n",
        "    updates = (1 - p_gen) * tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) \n",
        "    updates = tf.transpose(updates, perm=[0, 2, 1])\n",
        "    tf.print(tar_shape-1)\n",
        "    #shape = tf.Variable([batch, tar_shape-1, target_vocab_size]) \n",
        "    shape = [batch, tar_shape-1, target_vocab_size]\n",
        "    i1, i2 = tf.meshgrid(tf.range(batch),\n",
        "                     tf.range(inp_shape), indexing=\"ij\")\n",
        "    i1 = tf.tile(i1[:, :, tf.newaxis], [1, 1, tar_shape-1])\n",
        "    i2 = tf.tile(i2[:, :, tf.newaxis], [1, 1, tar_shape-1])\n",
        "    inp = tf.cast(encoder_input, dtype=tf.int32)\n",
        "    indices = tf.tile(inp[:, :, tf.newaxis], [1, 1, tar_shape-1])\n",
        "    indices = tf.stack([i1, i2, indices], axis=-1)\n",
        "    copy_logits = tf.scatter_nd(indices, updates, shape)   \n",
        "    combined_output = final_output + copy_logits\n",
        "    final_output = self.final_layer(combined_output)  \n",
        "    #tf.print(final_output.shape)\n",
        "    return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olO_bSrJHYvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Generator(tf.keras.Model):\n",
        "#   def __init__(self):\n",
        "#     super(Generator, self).__init__()\n",
        "#     self.generator_vec = tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "#     self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "#   def call(self, dec_output, final_output, attention_weights, encoder_input,training):  \n",
        "#     p_gen = self.generator_vec(dec_output) # (batch_size, tar_seq_len, 1)\n",
        "#     vocab_dist_ = p_gen * final_output\n",
        "#     updates = (1 - p_gen) * tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) \n",
        "#     updates = tf.transpose(updates, perm=[0, 2, 1])\n",
        "#     shape = tf.constant([BATCH_SIZE, summ_length-1, target_vocab_size]) \n",
        "#     copy_logits = tf.scatter_nd(indices, updates, shape)   \n",
        "#     combined_output = final_output + copy_logits\n",
        "#     final_output = self.final_layer(combined_output)  \n",
        "#     return final_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5ABqd0XBGxO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# i1, i2 = tf.meshgrid(tf.range(64),\n",
        "#                      tf.range(509), indexing=\"ij\")\n",
        "# i1 = tf.tile(i1[:, :, tf.newaxis], [1, 1, 68])\n",
        "# i2 = tf.tile(i2[:, :, tf.newaxis], [1, 1, 68])\n",
        "# inp = tf.cast(inp, dtype=tf.int32)\n",
        "# indices = tf.tile(inp[:, :, tf.newaxis], [1, 1, 68])\n",
        "# idx = tf.stack([i1, i2, indices], axis=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAv-fwCzxDM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator = Generator()\n",
        "\n",
        "# The @tf.function trace-compiles train_step into a TF graph for faster\n",
        "# execution. The function specializes to the precise shape of the argument\n",
        "# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n",
        "# batch sizes (the last batch is smaller), use input_signature to specify\n",
        "# more generic shapes.\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar, inp_shape, tar_shape, batch):\n",
        "  tar_inp = tar[:, :-1]\n",
        "  tar_real = tar[:, 1:]\n",
        "  \n",
        "  enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions, attention_weights, dec_output = transformer(inp, tar_inp, \n",
        "                                 True, \n",
        "                                 enc_padding_mask, \n",
        "                                 combined_mask, \n",
        "                                 dec_padding_mask)\n",
        "    final_predictions = generator(dec_output, predictions, attention_weights, inp, inp_shape, tar_shape, batch, True)\n",
        "    #tf.print(tar_real.shape, final_predictions.shape)\n",
        "    loss = loss_function(tar_real, final_predictions)\n",
        "\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "  \n",
        "  train_loss(loss)\n",
        "  train_accuracy(tar_real, predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkHTsSD4MmNZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "2c803b32-b7f3-4a8c-edf3-99f4ae1c900b"
      },
      "source": [
        "for (batch, (inp, tar)) in enumerate(val_dataset):\n",
        "  print(inp.shape,'  ', tar.shape)"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 509)    (64, 70)\n",
            "(64, 507)    (64, 70)\n",
            "(64, 510)    (64, 70)\n",
            "(64, 511)    (64, 70)\n",
            "(64, 509)    (64, 70)\n",
            "(64, 505)    (64, 70)\n",
            "(64, 511)    (64, 70)\n",
            "(64, 512)    (64, 69)\n",
            "(64, 509)    (64, 70)\n",
            "(64, 504)    (64, 70)\n",
            "(64, 503)    (64, 70)\n",
            "(64, 505)    (64, 70)\n",
            "(64, 504)    (64, 70)\n",
            "(64, 509)    (64, 69)\n",
            "(64, 501)    (64, 69)\n",
            "(64, 496)    (64, 68)\n",
            "(64, 512)    (64, 70)\n",
            "(64, 510)    (64, 70)\n",
            "(64, 511)    (64, 70)\n",
            "(64, 510)    (64, 70)\n",
            "(64, 512)    (64, 70)\n",
            "(64, 512)    (64, 70)\n",
            "(1, 263)    (1, 56)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hlw7Bv-1NlE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tar_inp = tar[:, :-1]\n",
        "# tar_real = tar[:, 1:]\n",
        "# enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "# predictions, attention_weights, dec_output = transformer(inp, tar_inp, \n",
        "#                                  True, \n",
        "#                                  enc_padding_mask, \n",
        "#                                  combined_mask, \n",
        "#                                  dec_padding_mask)\n",
        "# final_predictions = generator(dec_output, predictions, attention_weights, inp, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDEUbx9tzzMv",
        "colab_type": "code",
        "outputId": "ffc3cca3-9da7-4ce8-b088-ab2e1f09bda2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "  \n",
        "  train_loss.reset_states()\n",
        "  train_accuracy.reset_states()\n",
        "  \n",
        "  \n",
        "  for (batch, (inp, tar)) in enumerate(val_dataset):\n",
        "    if batch==0 and epoch ==0:\n",
        "      print('Time taken for feeding the data to the model is {} seconds'.format(time.time()-start)) #takes 25 mins \n",
        "    train_step(inp, tar, inp.shape[1], tar.shape[1], inp.shape[0])\n",
        "    \n",
        "    if batch % 50 == 0:\n",
        "      print ('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(\n",
        "          epoch + 1, batch, train_loss.result(), train_accuracy.result()))\n",
        "      \n",
        "  if (epoch + 1) % 1 == 0:\n",
        "    ckpt_save_path = ckpt_manager.save()  \n",
        "    print ('Saving checkpoint for epoch {} and batch at {} path is {}'.format(epoch+1,\n",
        "                                                                     batch,\n",
        "                                                                     ckpt_save_path))\n",
        "    if train_loss.result() == 0:\n",
        "      print('converged at epoch {} for model with parameters{}'.format(epoch+1, \n",
        "                                                     (num_layers, \n",
        "                                                     d_model, \n",
        "                                                     dff, \n",
        "                                                     num_heads)))\n",
        "      break\n",
        "    \n",
        "  print ('Epoch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch + 1, \n",
        "                                                train_loss.result(), \n",
        "                                                train_accuracy.result()))\n",
        "\n",
        "  print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time taken for feeding the data to the model is 3.0572967529296875 seconds\n",
            "69\n",
            "Epoch 1 Batch 0 Loss 6.9656 Accuracy 0.0002\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "68\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "68\n",
            "68\n",
            "67\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "55\n",
            "Saving checkpoint for epoch 1 and batch at 22 path is /content/temp/ckpt-1\n",
            "Epoch 1 Loss 6.8790 Accuracy 0.0001\n",
            "Time taken for 1 epoch: 151.39122486114502 secs\n",
            "\n",
            "69\n",
            "Epoch 2 Batch 0 Loss 6.5429 Accuracy 0.0000\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "68\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "68\n",
            "68\n",
            "67\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "55\n",
            "Saving checkpoint for epoch 2 and batch at 22 path is /content/temp/ckpt-2\n",
            "Epoch 2 Loss 6.4021 Accuracy 0.0000\n",
            "Time taken for 1 epoch: 75.30403089523315 secs\n",
            "\n",
            "69\n",
            "Epoch 3 Batch 0 Loss 6.1357 Accuracy 0.0000\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "69\n",
            "68\n",
            "69\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XS4y7Na-WeUn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJNJBexXz3fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(inp_sentence):\n",
        "  start_token = [tokenizer_en.vocab_size]\n",
        "  end_token = [tokenizer_en.vocab_size + 1]\n",
        "  \n",
        "  # inp sentence is portuguese, hence adding the start and end token\n",
        "  inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "  encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "  \n",
        "  # as the target is english, the first word to the transformer should be the\n",
        "  # english start token.\n",
        "  decoder_input = [tokenizer_en.vocab_size]\n",
        "  output = tf.expand_dims(decoder_input, 0)\n",
        "    \n",
        "  for i in range(MAX_LENGTH):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "        encoder_input, output)\n",
        "  \n",
        "    # predictions.shape == (batch_size, seq_len, vocab_size)\n",
        "    predictions, attention_weights, _ = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask)\n",
        "    temp = predictions\n",
        "    # select the last word from the seq_len dimension\n",
        "    predictions = predictions[: ,-1:, :]  # (batch_size, 1, vocab_size)\n",
        "\n",
        "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
        "    \n",
        "    # return the result if the predicted_id is equal to the end token\n",
        "    if predicted_id == tokenizer_en.vocab_size+1:\n",
        "      return tf.squeeze(output, axis=0), attention_weights\n",
        "    \n",
        "    # concatentate the predicted_id to the output which is given to the decoder\n",
        "    # as its input.\n",
        "    output = tf.concat([output, predicted_id], axis=-1)\n",
        "\n",
        "  return tf.squeeze(output, axis=0), attention_weights, temp, encoder_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsYNhAS0z8r6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def summarize(sentence):\n",
        "  result, attention_weights, logits, encoder_input = evaluate(sentence)\n",
        "  \n",
        "  predicted_sentence = tokenizer_en.decode([i for i in result \n",
        "                                            if i < tokenizer_en.vocab_size])  \n",
        "\n",
        "  print('Input: {}'.format(sentence))\n",
        "  print('Predicted translation: {}'.format(predicted_sentence))\n",
        "  return (result, attention_weights, logits, encoder_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47Ng55Qov4GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generator_vec = tf.keras.layers.Dense(1, activation='sigmoid' )\n",
        "# p_gen = generator_vec(gen_logits) # (batch_size, tar_seq_len, 1)\n",
        "# #vocab_dist_ = p_gen * final_output\n",
        "# # mean of all the attention heads\n",
        "# # updates shape (batch_size, tar_seq_len, inp_seq_len)\n",
        "# updates = (1 - p_gen) * tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1)\n",
        "# updates = tf.transpose(updates, perm=[2, 1, 0])\n",
        "# batch_size = updates.shape[-1]\n",
        "# # since the target input is shifted right so reducing the summary length\n",
        "# shape = tf.constant([target_vocab_size, summ_length-1, batch_size]) #rename batch_size when training \n",
        "# #shape = tf.constant([BATCH_SIZE, summ_length-1, target_vocab_size]) #rename batch_size when training \n",
        "# indices = tf.transpose(inp)#, perm=[1, 0])\n",
        "# indices = tf.cast(indices, dtype=tf.int32)\n",
        "# #indices = tf.expand_dims(indices, axis=1)\n",
        "# print('updates {}'.format(updates.shape))\n",
        "# print('indices {}'.format(indices.shape))\n",
        "# print('shape {}'.format(shape))\n",
        "# copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "# #copy_logits = tf.transpose(copy_logits_, perm=[2, 1, 0])\n",
        "# final_output = p_gen + copy_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFrQfOjm0AyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Greedy search output\n",
        "sample_document = '''Our pointer-generator network improved our ROUGE scores from our baseline; further analysis of the\n",
        "summaries outputted by this model show that while we successfully reduced OOV mishandling, we\n",
        "still suffer from factual inaccuracies. For instance, in the example below, it is clear that we no longer\n",
        "run into out of vocabulary words; indeed, the model uses words like \"yanukovych\", \"sergei\", and\n",
        "\"oleg\" in its summary, despite the fact that these are uncommon words that our baseline transformer\n",
        "could not generate'''\n",
        "result, attention_weights, gen_logits, encoder_input = summarize(sample_document)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l46didK4D9Rz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# add a dense layer (dff, 1) with sigmoid activation \n",
        "\n",
        "# take mean of all the attention heads\n",
        "updates = tf.reduce_mean(attention_weights['decoder_layer{}_block2'.format(num_layers)], axis=1) #(batch_size, tar_seq_len, inp_seq_len)\n",
        "updates = tf.transpose(updates, perm=[2 ,1, 0]) #(input_seq_len, tar_seq, batch)\n",
        "tar_seq_len = updates.shape[1]\n",
        "batch_size = updates.shape[-1]\n",
        "shape = tf.constant([target_vocab_size, tar_seq_len, batch_size]) #rename batch_size when training \n",
        "indices = tf.transpose(encoder_input)\n",
        "copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "copy_logits = tf.transpose(copy_logits_, perm=[2, 1, 0])\n",
        "logits = gen_logits + copy_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtB1nlbv62pV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pg6Rhv1EXN1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "updates.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXfZx0Y3XSB5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79E3L3lTXbf1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "indices.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXVIuTmR9zQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class Generator(tf.keras.Model):\n",
        "#   def __init__(self, dff):\n",
        "#     super(Generator, self).__init__()\n",
        "\n",
        "# #     self.encoder = Encoder(num_layers, d_model, num_heads, dff, \n",
        "# #                            input_vocab_size, rate)\n",
        "\n",
        "# #     self.decoder = Decoder(num_layers, d_model, num_heads, dff, \n",
        "# #                            target_vocab_size, rate)\n",
        "\n",
        "#     self.generator_vec = tf.keras.layers.Dense(dff, activation='sigmoid')\n",
        "    \n",
        "#   def call(self, dec_output, final_output, attention_weights, encoder_input, #encoder_input :-first argument of transformer _call\n",
        "#            training):  \n",
        "#     p_gen = self.generator_vec(dec_output)\n",
        "#     vocab_dist = tf.math.softmax(final_output, axis=-1)\n",
        "#     vocab_dist_ = p_gen * vocab_dist\n",
        "#     attn_dist_ = tf.math.softmax(attention_weights['decoder_layer{}_block2'.format(num_layers)], dim=-1)\n",
        "#     attn_dist_ = (1 - p_gen) * attn_dist_\n",
        "#     updates = tf.reduce_mean(attn_dist_, axis=1) #(batch_size, tar_seq_len, inp_seq_len)\n",
        "#     updates = tf.transpose(updates, perm=[2 ,1, 0])\n",
        "#     shape = tf.constant([target_vocab_size, summ_length, BATCH_SIZE]) #rename batch_size when training \n",
        "#     indices = tf.transpose(encoder_input)\n",
        "#     copy_logits_ = tf.scatter_nd(indices, updates, shape)   # output shape (target_vocab_size, tar_seq_len, batch_size)\n",
        "#     copy_logits = tf.transpose(copy_logits_, perm=[2, 1, 0])\n",
        "#     logits = p_gen + copy_logits\n",
        "    \n",
        "#     return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHxhdwc4-l_A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note remove pad code when training (the ids would be padded already by the padd_batch step of tf.data)\n",
        "#\n",
        "def beam_search_train(inp_sentences, beam_size):\n",
        "  \n",
        "  start = [tokenizer_en.vocab_size] * len(inp_sentences)\n",
        "  end = [tokenizer_en.vocab_size+1]\n",
        "  pad_token = 0\n",
        "  new = []\n",
        "  inp_sentences = [tokenizer_en.encode(i) for i in inp_sentences]\n",
        "  N = doc_length                                                         # set N as length of the sentence with max len\n",
        "  inp_sentences = [[i]*beam_size for i  in inp_sentences]\n",
        "  encoder_input = tf.reshape(tf.convert_to_tensor(inp_sentences), [-1, N]) # (batch_size * beam_size, doc_length)\n",
        "  def transformer_query(output):\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "          encoder_input, output)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 True,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask,\n",
        "                                                 True)\n",
        "\n",
        "    # select the last sequence\n",
        "    return (predictions[:,-1:,:]) # (batch_size, 1, target_vocab_size)\n",
        "  return beam_search(transformer_query, start, beam_size, 120, target_vocab_size, 0.6, stop_early=True, eos_id=[end])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAasC6jG1OdE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note remove pad code when training (the ids would be padded already by the padd_batch step of tf.data)\n",
        "def beam_search_eval(inp_sentences, beam_size):\n",
        "  \n",
        "  start = [tokenizer_en.vocab_size] * len(inp_sentences)\n",
        "  end = [tokenizer_en.vocab_size+1]\n",
        "  pad_token = 0\n",
        "  new = []\n",
        "  inp_sentences = [tokenizer_en.encode(i) for i in inp_sentences]\n",
        "  N = len(max(inp_sentences, key=len))  # set N as length of the sentence with max len\n",
        "  # Pad zero\n",
        "  inp_sentences = [[i + [pad_token] * (N - len(i))]*beam_size if len(i) < N else [i]*beam_size for i  in inp_sentences]\n",
        "  encoder_input = tf.reshape(tf.convert_to_tensor(inp_sentences), [-1, N])\n",
        "  \n",
        "  def transformer_query(output):\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "          encoder_input, output)\n",
        "    predictions, attention_weights = transformer(encoder_input, \n",
        "                                                 output,\n",
        "                                                 False,\n",
        "                                                 enc_padding_mask,\n",
        "                                                 combined_mask,\n",
        "                                                 dec_padding_mask,\n",
        "                                                 True)\n",
        "\n",
        "    # select the last sequence\n",
        "    return (predictions[:,-1:,:])  # (batch_size, 1, target_vocab_size)\n",
        "  return beam_search(transformer_query, start, beam_size, summ_length, target_vocab_size, 0.6, stop_early=True, eos_id=[end])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIWHvzz61fkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inp_sentences = [sample_document]\n",
        "beam_size = 7\n",
        "start_time = time.time()\n",
        "# translated_output_temp[0] =>  (batch, beam_size, summ_length+1)\n",
        "translated_output_temp = beam_search_eval(inp_sentences, beam_size)\n",
        "\n",
        "for true_summary, top_sentence_ids in zip(inp_sentences, translated_output_temp[0][:,0,:]):\n",
        "  print('Original summary: {}'.format(true_summary))\n",
        "  print('Predicted summary: {}'.format(tokenizer_en.decode([j for j in top_sentence_ids if j < tokenizer_en.vocab_size])))\n",
        "print('time to process {}'.format(time.time()-start_time))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAeibRXBvMNx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Logits needs to be returned from transformer query function\n",
        "# def beam_search_eval(inp_sentence):\n",
        "#   beam_size = 5\n",
        "#   start_token = [tokenizer_en.vocab_size]\n",
        "#   end_token = [tokenizer_en.vocab_size + 1]\n",
        "#   inp_sentence = start_token + tokenizer_en.encode(inp_sentence) + end_token\n",
        "#   encoder_input = tf.expand_dims(inp_sentence, 0)\n",
        "#   encoder_input = tf.concat([encoder_input]*beam_size, axis=0)\n",
        "#   start = tokenizer_ta.vocab_size\n",
        "#   end = tokenizer_ta.vocab_size+1\n",
        "#   def transformer_query(output):\n",
        "\n",
        "#     enc_padding_mask, combined_mask, dec_padding_mask = create_masks(\n",
        "#           encoder_input, output)\n",
        "#     predictions, attention_weights = transformer(encoder_input, \n",
        "#                                                  output,\n",
        "#                                                  False,\n",
        "#                                                  enc_padding_mask,\n",
        "#                                                  combined_mask,\n",
        "#                                                  dec_padding_mask)\n",
        "\n",
        "\n",
        "#     return (predictions[:,-1:,:])\n",
        "#   return beam_search(transformer_query, [start], beam_size, 120, target_vocab_size, 0.6, stop_early=False, eos_id=[tokenizer_ta.vocab_size+1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh41DoV2mmKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Add BERT weights : -computational issues\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_nLiI1AdPN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# histogram on number on number of total token length (doc+summary) vs training examples\n",
        "# Neat script with the same pipeline as Tf-toy train\n",
        "# add label smoothing\n",
        "# copy mechanism \n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}